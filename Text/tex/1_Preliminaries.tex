%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Foreword %%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcounter{reqcounter}[section]
\newcommand{\req}[2]{
\stepcounter{reqcounter}
\indent\par
\textbf{#1\arabic{reqcounter} #2}
}
\newcommand{\funcreq}[1]{\req{F}{#1}}
\newcommand{\nonfreq}[1]{\req{N}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functional requirements %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Binary search tree}

A binary search tree is a data structure that implements insert, remove and find operations on a set of keys. Key can be of any type, that has total ordering [ref to a total ordering explanation]. Throughout the thesis, trees with distinct keys are discussed, although of course, it is not the necessary condition.

A binary tree consists of nodes. A node consists of a key and two references to other nodes – left child (defined as left(N)) and right child (defined as right(N)). A reference may contain a link to an existing node or a special value nil, that means that the reference is empty. A node that contains a link to a child is called a parent of this child node. Nodes may contain other attributes as well, but those are not substantial for this explanation. A node N that has no children (both left(N) = nil and right(N) = nil) is called a leaf.

From the perspective of the graph theory, the binary tree is a simple oriented acyclic graph, where vertices are represented as nodes and edges are represented as links between a node and its left and right children. Every vertex in such graph has at most one incoming edge, i.e. every node can have at most one parent. Moreover, only one node has no parent. This node is called the root of a binary tree. The length of the longest path from any leaf to the root is known as a height of a binary tree. Subtree with a root in node N is N and a set of nodes that can be reached from N by child links.

Binary search tree (BST) is a binary tree that satisfies the following condition: for each node N, subtrees with a root in left(N) and with a root in right(N) contain only nodes with keys, that are less or equal than key(N) and larger or equal than key(N), respectively. Using this property, it is possible to implement a fast lookup of key k in a binary search tree, see listing 1:

find(T, k)
N := root(T)
while (N != nil) do
  if (key(N) = k) then
    return N
  if (key(N) < k) then
    N := left(N)
  else
    N := right(N)

return nil

It is easy to prove, that the complexity of this search algorithm is O(height(T)), assuming that key comparison takes O(1). Furthermore, two remaining operations of a binary tree, insert and remove, are implemented in the same fashion, with partial changes, and both of those operations have complexity O(height(T)). Their implementation is described in details in (Sedgewick and Wayne 2011).

It is clear that the performance of all operations on a BST directly depends on its height, that can vary between O(size(T)) (the case when every node, except the last, has exactly one child; a BST has the same structure as a linked list) and O(log(size(T)) in case of a complete binary tree. Therefore, a BST will maintain optimal operation time only if its structure is close to a complete binary tree and the height is close to a c * log(size(T)), where c is a constant factor greater or equal 1. To keep the height logarithmic, even in a worst-case scenario, the tree balancing has been invented. The idea is that a tree keeps track of its structure and if it is not optimal, then the rebalancing is applied to restore an optimal structure. The rebalancing can be achieved through the tree rotation [Sedgewick] – an operation, that swaps a node with its parent in a way, that preserves the BST property.


\section{AVL Tree}

AVL tree is a classic example of a self-balancing BST. Moreover, it provides one of the strongest guarantees on a tree height — distances from a root to every leaf differ by at most one. So in practice, the height of an AVL tree is always equal to ceil(log(size(T))). Other self-balanced trees usually provide a weaker guaranty — for example, they stand that the height is a multiplication of a log(size(T)) [Red-Black tree]. AVL tree was invented in 1962 by Georgy Adelson-Velsky and Evgenii Landis. Self-balancing is achieved with the following approach: every node holds a difference between the heights of its left and right subtrees; this difference is called balance factor. AVL property stands that every node has balance factor -1, 0 or 1. After every operation, that modifies tree structure – insert and remove, those balance factors are updated. If at any step the balance factor happens to be -2 or 2, a rotation (or a double rotation, as described in [2]) is applied. This rotation affects the heights of the left and right subtrees and, consequently, restores AVL property. The exact AVL tree implementation is described in [] and [].

\section{Splay Tree}

Another approach on tree balancing is presented in the Sleator and Tarjan work [] - “The efficiency of splay trees comes not from an explicit structural constraint, as with balanced trees, but from applying a simple restructuring heuristic, called splaying, whenever the tree is accessed.” By term splaying they mean the process of using rotations (similar to ones in AVL tree) to bring the last accessed node in the root. Sleator and Tarjan proved that by using this technique, all three basic operations (find, insert and remove) have a logarithmic time bound. Another benefit of splaying is that the most frequently accessed elements tends to gather near the root, therefore improving access speed, especially on skewed input sequences – the sequences, in which only a small number of items are accessed the most while other items occur rarer.

Even though splay trees show several interesting theoretical properties, in practice they are outperformed by more conventional BSTs, like AVL and Red-Black tree. This is due to the fact that in a splay tree the structure of the tree is altered on every operation, including find operation, while AVL, for instance, modifies the tree only during insertions and removals. A typical use scenario for those data structures is a scenario, where a vast majority of operations is search operations, while updates are not so often. AVL and Red-Black trees happen to be faster because they execute fewer instructions per find operation. Moreover, they do not make any writes to memory during look-up, and as a consequence, there is lower load on the memory bus and system cache. Further researches on splay trees were focused in the main on how to reduce the number of rotations during splaying. An extensive overview of those optimizations is provided in []. One of the described techniques, the partial splaying is a modification of a conventional splay tree, where every node contains a counter that denotes a total count of accesses to this node. As usual, splaying is performed on every access, but the node is splayed only until its access count exceeds an access count of its parent. W. Klostermeyer showed that this modification does not gain any noticeable advantage over a standard splay tree. However, partial splaying and other derived modifications can have some peculiar properties specifically in application to a numerical database. It will be discussed in section 4.

\section{Hash Table}

Hash table is another popular data structure that implements dictionary abstract data type. It uses an entirely different approach on item storage and lookup. A hash table allocates a contiguous array A, which size is bounded by the expected number of items to be stored, often multiplied by a factor alpha, that is called a load factor.

Firstly, let’s look at a simplified case: the key that is used in a hash table is of an integer type. Having an array A (mentioned previously), a key K and a value V, associated with K, it is possible to use a remainder of the division of K by the size of A as an index in A and store V in A at this index. This would give the best performance possible, as the value is retrieved immediately and lookup-time does not depend on the total count of items in the hash table. However, since the modulo operation has been used, there can be several keys that point at the same index in A. This circumstance is called collision.

To deal with the collision, it is necessary to store K itself together with V, so that in case of a collision it would be possible to tell if the stored V is actually associated with a particular key K or another key, that collides with K. Secondly, one must pick a strategy on how to deal with the case when two different key, K1 and K2, that point at the same index are inserted. There are two ways to deal with collisions:
Separate chaining (open hashing) - each element in A is a linked list (or another data structure), that stores all pairs <K, V> that happened to collide
Linear probing (closed hashing) – if during insertion of the key K in A at the index I a collision happens (I-th item in A is already occupied, a special algorithm F is used to determine the second index at which K can be inserted. If it is also occupied, 3rd and all consequent positions, generated with F, are used to try to insert an element.
Sedgewick and Wayne provide a detailed explanation of those strategies and a description of hash tables in general in [Sedgewick].

The approach described above can be generalized on keys of any type T. It is achieved using a hash function. This function takes an argument of type T and maps it to an integer (called hash). This function must satisfy two properties:
Deterministic – it should map the same object to the same hash every time.
Uniform or collision-free property– if used with a uniformly random sequence of objects of type T as input, the hash function should produce a uniform sequence of hashes.
More information about hash function properties and hash function construction is presented in [Knott].

\section{Coarse-grained Locking}

A trivial way to parallelize a sequential data structure is to eliminate a concurrent access at all. It can be achieved with a single mutual exclusion lock – mutex. While a thread holds a mutex, no other thread can lock the same mutex. The sequential data structure is wrapped into a helper class, that locks the mutex in the beginning of every operation and releases it in the end, so that only one thread can access the data structure at a time, no matter, how many core are involved. It is usually called the coarse-grained locking, in contrast with the fine-grained locking, where many locks are used and each lock protects only a part of the data structure, so other threads can freely access other parts.

Pros of this approach is a very trivial implementation and the absence of any special requirements on the underlying data structure. However, coarse-grained locking is only suitable when a data structure has a support role in a program and is used occasionally. If the data structure is the key element of the application, then a single lock becomes a bottleneck in the software, drastically decreasing program scalability. In this case one should use more sophisticated parallelization approaches.

\section{Binning}

The evolution of coarse-grained locking is the binning. The main drawback of the previous approach is that a single lock becomes the main point of contention between threads. One way to cope with this is to increase the number of locks. In contrast with the fine-grained locking, the binning does not involve any modifications of the underlying container.

Firstly, the numbers of bins – independent data structure instances – is chosen. Then a mapping between the item domain and a bin number is introduced. The mapping should yield a uniform distribution of mapped values. Every item is stored only in its assigned bin. Every bin has its own mutex, therefore the access to every bin is serialized. But since items are mapped uniformly, it is expected to produce much less contention than in case of a single lock.

\section{Fine-grained Locking}

The fine-grained locking usually offers better scalability, than the previously discussed approaches. Instead of a single lock, many mutexes are used simultaneously. Every mutex protects its part of data. The contention between threads is lower as it is unlikely several threads will access the same portion of data at the same time. However, this is true only if every portion has the same probability of being accessed (like in a concurrent hash table). In some data structures, typically binary trees, there are some nodes that are accessed (and are locked before access) oftener than others, e.g. a root in a binary tree. Substantial modifications must be made to a data structure to integrate the fine-grained locking. Fine-grained locking is not a silver bullet, but it offers a reasonable trade-off between implementation complexity and application scalability.


\newcommand{\uitem}[1]{
\item \textbf{#1} \par
}
\newcommand{\aitem}[1]{
\item \textbf{#1} \par
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Payments system %%%%%%%%%%%%%%%%%%%%%%%%%%%%%







