%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Foreword %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Functional requirements %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Binary search tree}

A binary search tree is a data structure that implements \findop, \insertop and, \removeop operations on a set of keys. Key~$K$ can be of any type, that has a total order. Throughout the thesis, trees with distinct keys are discussed, although of course, it is not the necessary condition.

A binary tree consists of nodes. A node~$N$ consists of a key (defined as~\func{key}{N}) and two references to other nodes~-- left child (defined as~\func{left}{N}) and right child (defined as~\func{right}{N}). A reference may contain a link to an existing node or a special value $nil$, that means that the reference is empty. A node that contains a link to a child is called a \emph{parent} of this child node. Nodes may contain other attributes as well, but those are not substantial for this explanation. A node that has no children (\(\func{left}{N} = nil \land \func{right}{N} = nil\)) is called a \emph{leaf}.

From the perspective of the graph theory, the binary tree is a simple oriented acyclic graph, where vertices are represented as nodes and edges are represented as links between a node and its left and right children. Every vertex in such graph has at most one incoming edge, i.e. every node can have at most one parent. Moreover, only one node has no parent~-- this node is called the \emph{root} of a binary tree. The length of the longest path from any leaf to the root is known as the \emph{height} of a binary tree. The \emph{subtree} with a root in $N$ defined as $N$ and a set of nodes that can be reached from $N$ by child links.

Binary \emph{search} tree ($BST$) is a binary tree that satisfies the following condition: for each $N$, subtrees with the root in \func{left}{N} and with the root in \func{right}{N} contain only nodes with keys, that are less or equal than \func{key}{N} and larger or equal than \func{key}{N}, respectively. Using this property, it is possible to implement a fast lookup of a key $K$ in a binary search tree, see listing 1:


\begin{algorithm}
\caption{Lookup in $BST$}\label{bst_find}
\begin{algorithmic}[1]
  \Procedure{Find}{$root,K$}\Comment{The node with key = $K$ or $nil$}
    \State $node\gets root$
    \While{$node\not= nil$}
    \If{$K = \func{key}{node}$}
      \State \textbf{return} $node$
    \ElsIf{$K < \func{key}{node}$}
      \State $node \gets \func{left}{node}$
    \Else \Comment{$K > \func{key}{node}$}
      \State $node \gets \func{right}{node}$
    \EndIf
    \EndWhile
    \State \textbf{return} $nil$\Comment{The node was not found}
  \EndProcedure
\end{algorithmic}
\end{algorithm}

It is easy to prove, that the complexity of this search algorithm is~$\mathcal{O}(\func{height}{T})$, assuming that key comparison takes~$\mathcal{O}(1)$. Furthermore, two remaining operations of a binary tree, \insertop and \removeop, are implemented in the same fashion, with partial changes, and both of those operations have complexity~$\mathcal{O}(\func{height}{T})$. Their implementation is described in detail in~\cite{sedgewick}.

It is clear that the performance of all $BST$ operations  directly depends on the tree height, that can vary between $\mathcal{O}(\func{size}{T})$, where~$\func{size}{T}$ is the count of nodes in~$T$,
%(the case when every node, except the last, has exactly one child; a $BST$ has the same structure as a linked list)
 and $\mathcal{O}(\log(\func{size}{T}))$ in case of a complete binary tree~\cite{complete_bt}. Therefore, a BST will maintain optimal operation time only if its structure is close to a complete binary tree and the height is close to a $c \times \log(\func{size}{T})$, where $c$ is a constant factor greater or equal~$1$. To keep the height logarithmic, even in a worst-case scenario, the \emph{tree rebalancing} has been invented. The idea is that a tree keeps track of its structure and if it is not optimal, then the rebalancing is applied to restore an optimal structure. The rebalancing can be achieved through the \emph{tree rotation}\cite{sedgewick}~-- the operation, that swaps a node with its parent in a way, that preserves the $BST$ property.


\section{AVL Tree}

AVL tree was invented in 1962 by Georgy Adelson-Velsky and Evgenii Landis\cite{avl_tree}. It is a classic example of a self-balancing $BST$. In fact, the height of the AVL tree is never greater than $1.4405\times \log(\func{size}{T}) - 0.3277$ \cite[p.~460]{knuth3}.

Self-balancing is achieved with the following approach: every node holds a difference between the heights of its left and right subtrees; this difference is called \emph{balance factor}.
AVL property stands that every node has balance factor $-1$, $0$ or~$1$. After every operation, that modifies the tree structure~-- \insertop and \removeop, balance factors are updated.
If at any step the balance factor happens to be $-2$ or $2$, a rotation or a double rotation is applied.
The rotation adjusts the heights of the left and right subtrees and, consequently, restores the AVL property. The exact AVL tree implementation is described in \cite{sedgewick} and \cite{cormen}.

\section{Splay Tree}

Another approach on tree balancing is presented in the Sleator and Tarjan work\cite{splay_tree} - ``The efficiency of splay trees comes not from an explicit structural constraint, as with balanced trees, but from applying a simple restructuring heuristic, called splaying, whenever the tree is accessed.'' By term \emph{splaying} they mean the process of using rotations (similar to ones in the AVL tree) to bring the last accessed node to the root. Sleator and Tarjan proved that by using this technique, all three basic operations (\findop, \insertop and \removeop) have a logarithmic time bound. Another benefit of splaying is that the most frequently accessed items tend to gather near the root, therefore improving access speed, especially on skewed input sequences~-- the sequences, in which only a small number of items are accessed often while other items occur rarer.

Even though splay trees show several interesting theoretical properties, in practice they are outperformed by more conventional BSTs, like AVL and Red-Black tree\cite{splay_overview}. This is due to the fact that in the splay tree the structure of the tree is altered on every operation, including find operation, while AVL, for instance, modifies the tree only during insertions and removals. A typical use scenario for those data structures is a scenario, where a vast majority of operations is search operations, while updates are not so often. AVL and Red-Black trees happen to be faster because they execute fewer instructions per find operation. Moreover, they do not make any writes to memory during the lookup, and, as a consequence, there is lower load on the memory bus and system cache. Further researches on splay trees were focused in the main on how to reduce the number of rotations during splaying. An extensive overview of those optimizations is provided in \cite{splay_overview}. One of the described techniques, the partial splaying is a modification of a conventional splay tree, where every node contains a counter that denotes a total count of accesses to this node. As usual, splaying is performed on every access, but the node is splayed only until its access count exceeds an access count of its parent. W. Klostermeyer showed that this modification does not gain any noticeable advantage over a standard splay tree \cite{partial_splaying}. However, partial splaying and other derived modifications can have some interesting properties specifically in application to a numerical database. It will be discussed in \cref{ch:alt}.

\section{Hash Table}

Hash table is another popular data structure that implements dictionary abstract data type. It uses an entirely different approach on item storage and lookup. A hash table allocates a contiguous array $A$, which size is bounded by the expected number of items to be stored, often multiplied by the \mbox{\emph{load~factor}~$\alpha$}.

Firstly, letâ€™s look at a simplified case: the key $K$ that is used in a hash table is of an integer type. Having $A$, $K$ and the value $V$, associated with $K$, it is possible to use a remainder of the division of $K$ by the size of $A$ as an index in $A$. Then, $V$ will be stored in $A$ at this index. This approach would give the best performance possible, as the $V$ can be retrieved immediately and \emph{the~lookup time does not depend on~the~total count of~items} in the hash table. However, since the $modulo$ operation has been used, there can be several keys that point at the same index in $A$. This circumstance is called \emph{collision}.

To deal with a collision, it is necessary to store $K$ itself together with $V$, so that in case of a collision it would be possible to tell if the stored $V$ is actually associated with the $K$ or another $K'$, that collides with $K$. Secondly, one must pick a strategy on how to deal with the case when two different keys, $K_1$ and $K_2$, that point at the same index are inserted. There are two ways to deal with collisions:
\begin{description}
\item[Separate chaining (open hashing)]-- each element in $A$ is a linked list (or another data structure), that stores all pairs $\langle K, V\rangle$ that collides.
\item[Linear probing (closed hashing)]-- if during insertion of $K$ in $A$ at the index $I$ a collision occurs ($I$ is already occupied), a special function $F$ is used to determine the second index at which $K$ can be inserted. If it is also occupied, 3rd and all consequent positions, generated by $F$, are used to try to insert the element.
\end{description}

The approach described above can be generalized on keys of any type $T$. It is achieved with the help of a \emph{hash function}. This function takes an argument of type $T$ and maps it to an integer, called \emph{hash}. This function must satisfy two properties:

\begin{block-description}
\blockitem[Determinism]-- it should \emph{always} map the same input to the same hash.
\blockitem[Uniformity]-- if used with a uniformly generated random sequence of objects on input, the hash function should produce a uniform sequence of hashes.
\end{block-description}

In \cite{sedgewick} Sedgewick and Wayne provide the detailed explanation of collision avoidance strategies as well as general information about hash tables. More information about hash function properties and hash function construction is presented in \cite{knott}.

\section{Coarse-grained Locking}

A trivial way to parallelize a sequential data structure is to eliminate a concurrent access at all. It can be achieved with a single mutual exclusion lock~-- \emph{mutex}.
While a thread holds a mutex, no other threads can lock the same mutex.

The sequential data structure is wrapped into the helper type, that locks the mutex in the beginning of every operation and releases it in the end, so that only one thread can access the data structure at a time, no matter how many threads are involved.
This approach is called the \emph{coarse-grained locking}, in contrast with the \emph{fine-grained locking}, where many locks are used and each lock protects only a part of the data structure, so other threads can freely access other parts.

Pros of this approach is a very trivial implementation and the absence of any special requirements on the underlying data structure. However, coarse-grained locking is only suitable when a data structure has a support role in the program and is used occasionally. If the data structure is the key element of the application, then a single lock becomes the bottleneck in the software, drastically decreasing program scalability. In this case one should use more sophisticated parallelization approaches.

\section{Binning}

The evolution of coarse-grained locking is the \emph{binning}. The main drawback of the previous approach is that a single lock becomes the main point of contention between threads. One way to cope with this is to increase the number of locks. In contrast with the fine-grained locking, the binning does not involve any modifications of the underlying container.

Firstly, the numbers of bins~-- independent data structure instances~-- is chosen. Then a mapping between the item domain and a bin number is introduced. The mapping should yield a uniform distribution of mapped values. Every item is stored only in its assigned bin. Every bin has its own mutex, therefore the access to every bin is serialized. But since items are mapped uniformly, it is expected to produce much less contention than in case of a single lock.

\section{Fine-grained Locking}

The fine-grained locking usually offers better scalability, than the previously discussed approaches. Instead of a single lock, many mutexes are used simultaneously. Every mutex protects its part of data. The contention between threads is lower as it is unlikely several threads will access the same portion of data at the same time. However, this is true only if every portion has the same probability of being accessed (like in a concurrent hash table). In some data structures, typically binary trees, there are some nodes that are accessed (and are locked before access) oftener than others, e.g. a root in a binary tree. \emph{Substantial modifications} got to be made to the data structure to integrate the fine-grained locking. Sometimes the overhead added by this approach is so big, that it brings to naught any potential speed-up. Fine-grained locking is not a silver bullet, but usually it offers a reasonable trade-off between implementation complexity and application scalability.
