\section{Priority}

\subsection{Improved WST Priority}
\label{sssec:improved_wst}
The weighted search tree priority representation, discussed in \cref{sec:wst_priority}, has some drawbacks that probably were irrelevant at the time when \cite{park90} was presented. The problem is that keeping the hit counter in only 24 bits (even more, the hit counter multiplied by the base priority) would result in an integer overflow sooner or later. For example, imagine the case when the base priority is the maximum possible~-- 255. Binary representation is \nsnum{000000FF}{16}. After 65793 hits, the priority will have its maximum value~-- \nsnum{FFFFFFFF}{16}. If another hit counter adjustment is made an overflow occurs, producing the value \nsnum{000000FE}{16}. Therefore, the maximum priority becomes very low, and even the base priority has been changed. There are at least two possible solutions:
\begin{description}
\item[Use larger counter]-- store the WST priority in a 64-bit integer. Then an overflow of a 56-bit counter is unlikely, not to say impossible~-- even with the maximal priority, an overflow will occur only after the 282578800148737th insertion. It would take days to make so many adjustments, even if the processor performs only these adjustments and nothing else (which is at least impractical). Nevertheless, a certain disadvantage is that the memory overhead per each node is increased by 4 bytes.
\item[Perform a saturated addition] when adjusting the hit counter (\cref{alg:wst_priority2}). The saturated addition is the addition that yields the expected result if no overflow occurs during the operation and the maximum number for the given operand size otherwise. The drawback of this method is the increased computation time.
\end{description}

The difference between these two methods is the common trade-off between time and space. Since the total count of items that can be stored in a database with the limited memory available is the crucial characteristic of a database, the preference is given to the \emph{saturated addition} method.
\newfloat{algorithm}{tbp}{lop}
\begin{algorithm}
\caption{$WST$ priority update with saturation}\label{alg:wst_priority2}
\begin{algorithmic}[1]
  \Procedure{UpdateSaturated}{$priority$}\Comment{4 bytes long unsigned integer}
    \State $base\_priority\gets priority \mathrel{\&} \nsnum{FF}{16}$
    \State $new\_priority\gets base\_priority$
    \Comment{8 bytes long unsigned integer}
    \State $new\_priority \gets new\_priority \times \nsnum{100}{16}$
    \State\Comment{Maximum possible result is \nsnum{FFFFFFFF00}{16}}
    \If{$new\_priority < \nsnum{FFFFFFFF}{16}$}
    \Comment {Maximum value for 4 bytes}
      \State $priority \gets new\_priority + base\_priority$
    \Else
      \State $priority \gets \nsnum{FFFFFF00}{16} + base\_priority$
    \EndIf
    \State \textbf{return} $priority$
  \EndProcedure
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%Simple priority?

\subsection{Priority Aging}
\label{sssec:priority_aging}
The WST priority has one more drawback~-- it is suitable only for static input distributions~-- distributions, which mean remains constant during the execution. However, if the mean is known beforehand it is possible to construct a static optimal BST\cite[p.~442]{knuth3}, that will be more efficient than any dynamic lookup data structure.

A numerical database with the WST priority performs poorly on the time-varying distribution~-- a distribution which mean changes over time~-- while this type of distributions is more common in the real world applications. The problem arises from the fact that the priority does not reflect when an item was accessed for the last time. The worst-case scenario is the following: an item is added to a database, then it is accessed frequently hence its priority rises to the maximum, and then is not used over a long time. During the runtime, several items like this can appear. Even though at some point they are not accessed anymore they are still the most valuable items from the perspective of the database hence they will be kept much longer than other items. This pollutes the database with elements that are stored but not used.

There are several ways to cope with the problem. First of all, it is possible to use an entirely \emph{different eviction strategy}, the one that is not based on the item priority. Some of these strategies are described in the following section. Another solution is to adjust a priority not only when the corresponding node is accessed but also when it is visited (during the lookup of another item). For example, when searching in a binary search tree, the priority of the node $N$ that is being searched is increased while priorities of nodes, lying on the path between the root of the tree and $N$, are degraded.

This mechanism may not be effective with the AVL tree because in the AVL tree the order of the nodes does not correlates with node priorities. However, it seems very promising in application to splay trees~-- by applying this mechanism, the nodes near the root can stay there if only they are constantly accessed.

\section{Alternative Eviction Policies}
The canonical weighted search tree always chooses the node with the minimum priority for the deletion. However, it is only one of many possible eviction strategies. Some other strategies are presented in this section. From general ones, like LRU, to those exploiting the lookup data structure internals to find the least valuable item.

\subsection{LRU Policy}
The Least-Recently-Used policy tracks every access to the items and sorts them by access order. Then it evicts the item that was not accessed for the longest time. The common implementation is based on a doubly-linked list. When an item is accessed its corresponding node in the LRU list is moved into the head of the list. Then the least-recently-used node is the one in the tail of the list. When a new item is added, it is inserted in the head of the LRU list.

\subsection{LFU Policy}
\label{sssec:lfu}
The Least-Recently-Used item policy fulfills the same purpose as the LRU policy. But when it decides which item should be evicted, the access frequency is also taken in account in addition to the last access time (LRU uses only the latter property).

\subsection{Splay Policy}
\label{sssec:spolicy}
A splay tree tends to keep the most frequently accessed items near its root. By relying on this property, it is possible to eliminate a separate data structure that manages item priorities. When an eviction is performed, one of the bottom nodes is chosen for eviction. Even though this strategy may not choose an optimal node every time, it is expected to perform effectively on average. This approach yields the lowest memory overhead per node among all tested data structures.


\section{Alternative Sequential Containers}

\subsection{Hash Table}
One of the data structures that can be used in place of a weighted search tree is the hash table. Hash tables have faster than balanced BSTs lookup time under most workloads. What is more, a node in a hash table has lower memory overhead than a binary tree~-- using open hashing based on a double linked list every node stores only 2 pointers compared to 3 in a binary tree node and using closed hashing implies that no pointers are stored at all.

However closed hashing can not be used because when the hash table is almost full a lot of unsuccessful probes occur before a suitable index is found. Usually, this problem is solved by rehashing~-- if the count of probes exceeds a certain limit, the hash table is expanded. However, it is impossible in the numerical database since the amount of available memory is preset and cannot be exceeded.

On the other hand, limited memory is rather an advantage for the open hashing. If the total amount of memory available is known beforehand, then a hash table with open hashing can be preallocated to its maximum size and be never rehashed after. This, in turn, allows a concurrent version of a hash table to be simplified as the concurrent rehashing is one of the hardest problems to cope with.

\subsection{Splay Tree}
Another data structure that looks promising is the splay tree.
As it was mentioned in \cref{sec:splay}, usually splay trees tend to be slower than AVL. However, in application to the numerical database, it is possible to exploit the fact that the least valuable nodes are usually gathered in leaves of the tree. Therefore it is is possible to eliminate a binary heap from a numerical database and to use the splay policy, as described in \cref{sec:spolicy}. What is more, it is possible to implement a numerical database using a concurrent splay tree\cite{cb_tree}.

\section{Alternative Concurrent Containers}
\label{sec:concurrent_containers}
\subsection{Coarse-grained Lock Adapter}
The \numdbname library provides a universal adapter, that wraps a sequential container and protects it by using a coarse-grained lock. It has the same interface, as a usual numerical database container. All methods follow the same structure:
\begin{enumerate}
\item the mutex is locked
\item the call is forwarded to the underlying container
\item the mutex is released
\end{enumerate}

\subsection{Binning Adapter}
The binning adapter class realises the binning concept (described in \cref{sec:pre_bin}). It is very similar to the coarse-grained lock adapter, however, it encapsulates several instances of a container and several mutexes.

The number of bins is passed in the class constructor. The mapping is defined as  $\func{hash}{K}\bmod \texttt{bin count}$. Every bin is represented by a sequential container, e.g. the weighted search tree. In order to preserve the memory limit, all available memory is equally divided between all bins.

\subsection{CBDC}
For the purposes of the \numdbname library, the original concurrent container, called \emph{Concurrent Numerical Database Container}~-- \cndcname, has been developed. It defines 3 thread-safe operations~-- \findop, \insertop, and \func{removeMin}{}. Thread-safeness is achieved through the fine-grained locking approach. \cndcname is based on concurrent versions of the hash table and the binary heap. Sequential benchmarks (\cref{sec:seqbench}) proved that the combination of a hash table and a binary heap outperforms numerical databases, that are based on the LRU and LFU eviction strategies.

Fine-grained locking hash table implementation is much simpler compared to a similar concurrent BST. A lock is assigned to every hash table bucket (or every $k$ buckets) and every operation inside the bucket requires to lock the corresponding mutex. Since an operation in one bucket never interferes with any other buckets, only one lock is needed per operation, while other threads can operate on other buckets at the same time. Therefore, the overhead added by locking is much smaller, than in BST, where up to $\log{\func{height}{T}}$ mutexes has to be locked on every operations.

There are several known binary heaps with fine-grained locking (\cite{concurrent_heap1}, \cite{champ} and more). The \cndcname is based on the \libname{champ} binary heap, developed by Tamir, Morrison, and Rinetzky \cite{champ}. Unlike the majority of concurrent binary heaps, \libname{champ} allows priorities to be updated after an insertion.

In the following section, two types of locks are distinguished~-- the \emph{bucket} mutex (the one, that protects a single bucket in a hash table) and the \emph{heap} mutex (the one, that protects a single item in a heap. Every hash table node has a link to the corresponding heap node and vice versa. \cndcname operations are defined as follows:


\begin{block-description}
\item[\findop] consists of the following steps. At first, calling thread locks the corresponding bucket mutex. The requested item is searched in the bucket.

If the item is found, its priority is updated.
Before updating the corresponding heap mutex must be locked.
After locking the heap mutex the link to the heap node is checked again. If it has changed, the heap lock is released and the operation is repeated. Double check is required, because another thread can change the link even in case it does not hold the bucket mutex.

However, the heap mutex is required to be locked prior to the link update. Therefore, when a thread holds a heap mutex it is guaranteed, that no other thread can change the link between the heap node and the hash table node.

When the node is locked, the priority is updated and the \func{bubbleDown}{} (as defined in \cite{champ}) is performed. \func{bubbleDown}{} internally releases the heap and bucket locks.

\item[\insertop] has a structure, similar to \findop.
The corresponding bucket mutex is locked.
New item is inserted into the hash table. After that, the item is inserted in the binary heap (at the last index). Before the insertion is performed, the heap lock of the last index is locked.

The bucket mutex is released. It is possible to do this so early, because the \emph{heap} lock will be held till the end of the operation. While it is locked, no other thread can execute any operation on the same node.

Finally, \func{bubbleUp}{}\cite{champ} is performed. It propagates the node down until the heap invariant is restored.

\blockitem[\func{removeMin}{}] evicts the item with the lowest priority. This operation is decomposed into three independent parts.

\begin{enumerate}
\item The item is evicted from the heap.
\item The hash table node is marked as \emph{deleted}. Otherwise, another thread can access the item and start the priority update routine. Since the node does not exist in the heap anymore, the thread will enter into an infinite loop. Marking solves the problem as follows~-- the marked node can still be accessed by other threads, however, they will skip the priority update stage for the node.
\item The item is removed from the hash table.
\end{enumerate}

\end{block-description}
Search operation can be improved by using some technique, that would allow to release the bucket lock earlier, prior to \func{BubbleDown}{}. Such technique has not been developed yet.

