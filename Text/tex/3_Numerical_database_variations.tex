\section{Priority}

\subsection{Improved WST Priority}
The weighted search tree priority representation, discussed in [], has some drawbacks that probably were irrelevant at the time when the paper was presented. The problem is that keeping the hit counter in 24 bits only (even more, the hit counter multiplied by the base priority) would result in an integer overflow sooner or later. For example, imagine the case when the base priority is the maximum possible – 255. COmplete binary representation is 0x000000FF. After 65793 hits, the priority will have its maximum value – 0xFFFFFFFF. If another hit counter adjustment is made an overflow occurs, producing the value 0x000000FE. Therefore, the maximum priority becomes very low, and even the base priority is now changed. There are at least two possible solutions:
Store the WST priority in a 64-bit integer. Then an overflow of a 56-bit hit frequency counter is unlikely, not to say impossible – even with the maximal priority, an overflow will occur only after the 282578800148737th insertion. It would take days to make so many adjustments, even if the processor performs only these adjustments and nothing else (which is at least impractical). Nevertheless, a certain disadvantage is that the memory overhead per each node is increased by 4 bytes.
Perform a saturated addition when adjusting the hit counter. The saturated addition is the addition that yields the expected result if no overflow occurs during the operation and the maximum number for the given operand size otherwise. The drawback of this method is the increased computation time.

So the difference between those two methods is the common trade-off between time and space. Since the total count of items that can be stored in a database with the limited memory available is the crucial characteristic of a database, the preference is given to the second method.

%%%%%%%%%%%%Simple priority?

\subsection{Priority Aging}
The WST priority has one more drawback – it is suitable only for static input distributions – distributions, which mean remains constant during the execution. However, if the mean is known beforehand it is possible to construct a static optimal BST [], that will be more efficient than any dynamic look-up data structure.

A numerical database with the WST priority performs poorly on a time-varying distribution – a distribution which mean changes over time – while this type of distributions is much more common in the real world applications. The problem arises from the fact that the priority do not reflect when an item was accessed for the last time. The worst-case scenario is the following: an item is added to a database, then is frequently used so that its priority rises to the maximum, and then is not used over a long time. During the runtime, several items like this can appear. Even though at some point they are not used anymore they are the most valuable items from the perspective of the database hence will be kept much longer than other items. This pollutes the database with elements that are stored but not used.

There are several ways to cope with the problem. First of all, it is possible to use an entirely different eviction strategy, the one that is not based on an item priority. Some of these strategies are described in the following section. Another solution is to adjust a priority not only when the corresponding node is accessed but also when it is visited. For example, when searching in a binary search tree, the priority of the node x that is being searched is increased while priorities of nodes, lying on the path between the root of the tree and x, are degraded.

This mechanism may not be effective with an AVL tree because in an AVL tree the order of the nodes does not correlates with node priorities. However, it seems very promising in application to splay trees – by applying this mechanism, the nodes near the root can stay there if only they are constantly accessed.

\section{Alternative Eviction Policies}
A weighted search tree always chooses the node with the minimum priority for a deletion. However, it is only one of many possible eviction strategies. Some other strategies are presented in this section. From general ones, like LRU, to those exploiting the lookup data structure internals to find the least valuable item.

\subsection{LRU Policy}
The Least-Recently-Used policy tracks every access to the items and sorts them by access order. Then it evicts the item that was not accessed for the longest time. The common implementation is based on a doubly-linked list. When an item is accessed its corresponding node in the LRU list is moved into the head of the list. Then the least-recently-used node is the one in the tail of the list. When a new item is added, it is inserted in the head of the LRU list.

\subsection{LFU Policy}
The Least-Recently-Used item policy fulfills the same purpose as the LRU policy. But when it decides which item should be evicted, the access frequency is also taken in account in addition to the last access time (LRU uses only the latter property).

\subsection{Splay Policy}
A splay tree tends to keep the most frequently accessed items near its root. By relying on this property, it is possible to eliminate a separate data structure that manages item priorities. When an eviction is performed, one of the bottom nodes is chosen for eviction. Even though this strategy may not choose an optimal node every time, it is expected to perform effectively on average. This approach yields the lowest memory overhead per node among all tested data structures.


\section{Alternative Sequential Containers}

\subsection{Splay Tree}
One of the data structures that can be used in place of a weighted search tree is a splay tree. As it was mentioned in the section [], usually splay trees tend to be slower than AVL. However, in application to a numerical database, it is possible to exploit the fact that the least valuable nodes are usually gathered in leaves of the tree. Therefore it is is possible to eliminate a binary heap from a numerical database and to use the splay policy, as described in []. What is more, it is possible to implement a numerical database using a concurrent splay tree [].

\subsection{Hash Table}
Another data structure that looks promising is a hash table. Hash tables have faster than balanced BSTs lookup time under most workloads. What is more, a node in a hash table has lower memory overhead than a binary tree – using open hashing based on a double linked list every node stores only 2 pointers compared to 3 in a binary tree node and using closed hashing implies that no pointers are stored.

However closed hashing can not be used because when the hash table is almost full a lot of unsuccessful probes occur before a suitable index is found. Usually, this problem is solved by expanding and rehashing the whole hash table if the count of probes exceeds a certain limit, the whole hash table is expanded and rehashed. However, it is impossible in a numerical database since the amount of available memory is predefined and cannot be exceeded.

If the total amount of memory available is known beforehand, then a hash table with open hashing can be preallocated to its maximum size and be never rehashed after. This, in turn, allows a concurrent version of a hash table to be simplified as the concurrent rehashing is one of the hardest problems to cope with.



\section{Alternative Concurrent Containers}

\subsection{Coarse-grained Lock Adapter}
The Numdb library provides a universal adapter, that wraps a sequential container and protects it by using a coarse-grained lock. It has the same interface, as a usual numerical database container. All methods follow the same structure – at first, the mutex is locked, then the call is forwarded to the underlying container, then the mutex is released.

\subsection{Binning Adapter}
The binning adapter class realises the binning concept (described in[]). It is very similar to the coarse-grained lock adapter, however, it encapsulates several instances of a container and several mutexes.

The number of bins is passed in the class constructor. The mapping is defined as a hash value of K modulo bin count. Every bin is represented by a sequential container, e.g. the Weighted Search Tree. In order to maintain the memory limit, all available memory is equally divided between all bins.

\subsection{CBDC}
For the purposes of the numdb library, the original concurrent container, called CNDC (concurrent numerical database container), has been developed. It defines 3 thread-safe operations – find(), insert(), extractMin(). Thread-safeness is achieved through the fine-grained locking approach. CNDC is based on concurrent versions of the hash table and the binary heap. Sequential benchmarks [] proved that the combination of a hash table and a binary heap (that is used to manage priorities) outperforms numerical databases, that are based on the LRU and LFU eviction strategies.

Fine-grained locking hash table implementation is much simpler comparing to a similar concurrent BST. A lock is assigned to every hash table bucket (or every k buckets) and every operation inside the bucket requires to lock the corresponding mutex. Since an operation in one bucket never interferes with any other bucket, only one lock is needed per operation, while other threads can operate on other buckets at the same time. Therefore the overhead, added by locking, is much smaller, than in BST, where up to log(N) mutexes has to be locked on every operations.

There are several known binary heaps with fine-grained locking [], [], []. The CNDC is based on the CHAMP binary heap, developed by Tamir, Morrison, and Rinetzky. Unlike the majority of concurrent binary heaps, CHAMP allows priorities to be updated after an insertion.

In the following description, two types of locks are distinguished – the bucket mutex (the one, that protects a single bucket in a hash table) and the heap mutex (the one, that protects a single item in a heap. Every hash table node has a link to the corresponding heap node and vice versa. CNDC operations are defined as follows:
find
Calling thread locks the corresponding bucket mutex.
The requested item is searched in the bucket.
If found, item priority is updated
The corresponding heap node is locked
After locking the heap lock the link to the heap node is checked again. If it has changed, the heap lock is released and the operation is repeated. Double check is required, because another thread can change the link even in case it doesn’t hold the bucket mutex. However, the heap mutex must be locked prior to the link update. Therefore, when a thread holds a heap mutex it is guaranteed, that no other thread can change a link between heap and hash table nodes.
When the node is locked, the priority is updated and the BubbleDown operation \(as defined in []\) is performed
BubbleDown internally releases the heap lock
The bucket lock is released
insert has a structure, similar to the find operation
The corresponding bucket mutex is locked
New item is inserted into the hash table
The item is inserted in the binary heap \(at the last index\). Before the insertion is performed, the heap lock of the last index is locked.
Bucket mutex is released
BubbleUp operation \(as defined in []\) is performed
evictMin\(\) – evicts the item with the lowest priority. This operation is decomposed in two independent parts.
At first, the item is evicted from the heap.
After that the item is removed from the hash table.

Search operation can be improved by using some technique, that would allow to release the bucket lock earlier, prior to BubbleDown() operation. Such technique has not been developed yet.

